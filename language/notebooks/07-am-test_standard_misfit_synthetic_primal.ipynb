{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam, SGD\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from src.params import DatasetName, DEFAULT_DEVICE, CPU_GENERATOR\n",
        "from src.datasets import load_dataset, TRANSFORM_RGB, TRANSFORM_BASE\n",
        "from src.models import MLP, TruncatedMLP, PrimalThreshold, HomogenousMixtureModel, BagOfDecisionBoundaries\n",
        "from src.datasets import SyntheticDataSampler\n",
        "from src.engine import DatasetTrainer, SamplingTrainer, DatasetInference\n",
        "from src.metrics import cross_entropy_with_logits, logistic_bregman, reverse_loss, accuracy_with_logits, kl_divergence\n",
        "\n",
        "print(DEFAULT_DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize the hidden representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name = DatasetName.MNIST\n",
        "\n",
        "transform = T.Compose([T.ToTensor(), T.Lambda(lambda x: torch.flatten(x)), T.Lambda(lambda x: (x - x.mean()) / x.std()),])\n",
        "num_classes = 10\n",
        "target_transform = lambda x: F.one_hot(torch.tensor(x), num_classes=10).to(torch.float32)\n",
        "# num_classes = 2\n",
        "# target_transform = lambda x: torch.tensor([x <= 4, x > 4]).to(torch.float32)\n",
        "train_dataset = load_dataset(dataset_name, split=\"train\", transform=transform, target_transform=target_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 60000\n",
            "Number of classes: 10\n",
            "Sample dim: torch.Size([784])\n",
            "Target shape: torch.Size([10])\n",
            "Sample Mean: -8.089201486427555e-08\n",
            "Sample Std: 0.9999999403953552\n"
          ]
        }
      ],
      "source": [
        "# Get some stats on the mnist dataset \n",
        "print(f\"Number of samples: {len(train_dataset)}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "\n",
        "tmp_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "sample = next(iter(tmp_loader))\n",
        "\n",
        "data_sample_dim = sample[0].shape[1:]\n",
        "print(f\"Sample dim: {data_sample_dim}\")\n",
        "print(f\"Target shape: {sample[1].shape[1:]}\")\n",
        "print(f\"Sample Mean: {sample[0].mean()}\")\n",
        "print(f\"Sample Std: {sample[0].std()}\")\n",
        "# sample[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_dim = data_sample_dim[0]\n",
        "hidden_dim = 50\n",
        "target_num_layers = 2\n",
        "gt_model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim= num_classes, num_layers=target_num_layers)\n",
        "gt_representation = gt_model.representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model device for training: cpu\n",
            "Using model device for training: cpu\n",
            "Initializing the ground truth model representation on mnist\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:44<00:00,  8.97s/it, loss=0.105, up_norm=0.0491, acc=0.967] \n"
          ]
        }
      ],
      "source": [
        "gt_features_optimizer = Adam(gt_model.parameters(), lr=1e-3)\n",
        "gt_features_trainer = DatasetTrainer(gt_model, optimizer=gt_features_optimizer, loss_fn=cross_entropy_with_logits, metrics=[accuracy_with_logits], dataset=train_dataset)\n",
        "\n",
        "print(f\"Initializing the ground truth model representation on {dataset_name}\")\n",
        "gt_features_trainer.train(num_epochs=5, batch_size=256, average_window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model device for synthetic data generation: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[-0.2233,  0.5832,  1.9655,  ..., -0.1632,  0.4949, -1.5997],\n",
              "         [ 0.6048,  1.5956,  0.1239,  ..., -0.4499, -0.4875,  0.5903],\n",
              "         [ 0.0948,  1.1714,  0.2398,  ...,  0.8020, -0.3249,  0.4773],\n",
              "         ...,\n",
              "         [ 1.2762, -0.1854,  0.1299,  ...,  0.5097,  0.3059, -0.3169],\n",
              "         [ 1.2841,  0.4792, -0.2305,  ..., -1.3562, -0.3889, -1.1014],\n",
              "         [ 0.5049,  0.0780, -1.0979,  ..., -1.1325,  1.3271, -0.0626]]),\n",
              " tensor([[0.0449, 0.1271, 0.1839, 0.1913, 0.0346, 0.1404, 0.0289, 0.1373, 0.0579,\n",
              "          0.0537],\n",
              "         [0.0863, 0.0697, 0.1057, 0.3563, 0.0461, 0.1863, 0.0244, 0.0871, 0.0170,\n",
              "          0.0211],\n",
              "         [0.0027, 0.5544, 0.0182, 0.0245, 0.0402, 0.2186, 0.0192, 0.0811, 0.0113,\n",
              "          0.0298],\n",
              "         [0.0034, 0.1765, 0.0232, 0.3806, 0.0054, 0.2754, 0.0109, 0.0057, 0.1050,\n",
              "          0.0140],\n",
              "         [0.0171, 0.1994, 0.3994, 0.0759, 0.0533, 0.1144, 0.0329, 0.0232, 0.0532,\n",
              "          0.0313],\n",
              "         [0.0469, 0.0659, 0.1420, 0.1261, 0.0543, 0.3015, 0.1679, 0.0782, 0.0091,\n",
              "          0.0082],\n",
              "         [0.0295, 0.0264, 0.0119, 0.0112, 0.1394, 0.1987, 0.0381, 0.0541, 0.1021,\n",
              "          0.3887],\n",
              "         [0.0031, 0.2476, 0.0509, 0.0241, 0.0383, 0.4980, 0.0851, 0.0149, 0.0246,\n",
              "          0.0134],\n",
              "         [0.0308, 0.0913, 0.2159, 0.1118, 0.0287, 0.0267, 0.0160, 0.1637, 0.2153,\n",
              "          0.0997],\n",
              "         [0.0735, 0.0495, 0.0488, 0.1221, 0.0136, 0.4567, 0.0451, 0.1360, 0.0294,\n",
              "          0.0252]]),\n",
              " tensor([[-0.8397,  0.1998,  0.5691,  0.6085, -1.1012,  0.2990, -1.2812,  0.2773,\n",
              "          -0.5872, -0.6610],\n",
              "         [-0.0225, -0.2367,  0.1799,  1.3954, -0.6507,  0.7470, -1.2852, -0.0132,\n",
              "          -1.6491, -1.4301],\n",
              "         [-3.0709,  2.2511, -1.1665, -0.8679, -0.3740,  1.3205, -1.1099,  0.3283,\n",
              "          -1.6420, -0.6721],\n",
              "         [-2.6134,  1.3343, -0.6951,  2.1028, -2.1554,  1.7792, -1.4526, -2.1046,\n",
              "           0.8151, -1.2027],\n",
              "         [-1.6561,  0.8005,  1.4954, -0.1654, -0.5193,  0.2450, -1.0009, -1.3486,\n",
              "          -0.5209, -1.0523],\n",
              "         [-0.6662, -0.3268,  0.4406,  0.3221, -0.5209,  1.1939,  0.6084, -0.1560,\n",
              "          -2.3099, -2.4152],\n",
              "         [-1.1950, -1.3045, -2.0990, -2.1654,  0.3584,  0.7127, -0.9395, -0.5890,\n",
              "           0.0467,  1.3837],\n",
              "         [-3.0122,  1.3524, -0.2303, -0.9775, -0.5133,  2.0511,  0.2849, -1.4586,\n",
              "          -0.9576, -1.5673],\n",
              "         [-1.2763, -0.1898,  0.6714,  0.0129, -1.3453, -1.4172, -1.9285,  0.3943,\n",
              "           0.6686, -0.1010],\n",
              "         [-0.2932, -0.6886, -0.7025,  0.2138, -1.9816,  1.5328, -0.7830,  0.3216,\n",
              "          -1.2096, -1.3642]]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "var = 1.0\n",
        "gt_model.finetune_scale = 1.\n",
        "gt_data_sampler = SyntheticDataSampler(model=gt_model, input_dim=input_dim, output_dim=num_classes, var=var)\n",
        "gt_data_sampler.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now train primal model\n",
        "features_model = BagOfDecisionBoundaries(input_dim=hidden_dim, output_dim=hidden_dim * 100, num_states=num_classes) \\\n",
        "    .prepend(gt_representation, input_dim=input_dim, output_dim=hidden_dim)\n",
        "# features_model = PrimalThreshold(thresholds).prepend(gt_representation, input_dim=input_dim, output_dim=hidden_dim)\n",
        "gt_primal_model = HomogenousMixtureModel(\n",
        "    features_model=features_model,\n",
        "    output_dim=num_classes,\n",
        "    no_features_grad=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model device for training: cpu\n",
            "Using model device for training: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [04:02<09:25, 80.83s/it, loss=1.98, up_norm=0.038, acc=0.473] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m gt_primal_optimizer \u001b[38;5;241m=\u001b[39m SGD(gt_primal_model\u001b[38;5;241m.\u001b[39mmixture_layer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m gt_primal_trainer \u001b[38;5;241m=\u001b[39m DatasetTrainer(gt_primal_model, optimizer\u001b[38;5;241m=\u001b[39mgt_primal_optimizer, loss_fn\u001b[38;5;241m=\u001b[39mcross_entropy, metrics\u001b[38;5;241m=\u001b[39m[accuracy], dataset\u001b[38;5;241m=\u001b[39mtrain_dataset)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mgt_primal_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/pythagoras-agi-new/src/engine/trainer.py:260\u001b[0m, in \u001b[0;36mDatasetTrainer.train\u001b[0;34m(self, num_epochs, batch_size, average_window, update_pbar_every)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tensors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_data_loader):\n\u001b[1;32m    257\u001b[0m     data_batch \u001b[38;5;241m=\u001b[39m LabeledDataBatch\u001b[38;5;241m.\u001b[39mread_data_batch(\n\u001b[1;32m    258\u001b[0m         tensors, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    259\u001b[0m     )\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     ra\u001b[38;5;241m.\u001b[39madd(result\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    262\u001b[0m     update_norm_ra\u001b[38;5;241m.\u001b[39madd(result\u001b[38;5;241m.\u001b[39mscaled_step_norm\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[0;32m~/dev/pythagoras-agi-new/src/engine/trainer.py:66\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, data_batch)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(out, data_batch\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m---> 66\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# grad_norm = torch.norm(\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#     torch.cat([p.grad.flatten() for p_group in self.optimizer.param_groups for p in p_group[\"params\"]]), p=2  # type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "File \u001b[0;32m~/miniforge3/envs/pagi/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/pagi/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/pagi/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Test training the primal model on the actual data\n",
        "gt_primal_optimizer = SGD(gt_primal_model.mixture_layer.parameters(), lr=1)\n",
        "gt_primal_trainer = DatasetTrainer(gt_primal_model, optimizer=gt_primal_optimizer, loss_fn=cross_entropy_with_logits, metrics=[accuracy_with_logits], dataset=train_dataset)\n",
        "\n",
        "gt_primal_trainer.train(num_epochs=10, batch_size=256, average_window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inference = DatasetInference(gt_primal_model, train_dataset)\n",
        "res = inference.inference(batch_size=256)\n",
        "accuracy_with_logits(res, train_dataset.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model device for training: cpu\n",
            "Initializing the ground truth mixture parameters on mnist\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 203/3907 [00:54<16:38,  3.71it/s, loss=0.206, up_norm=0.108] \n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "index -1 is out of bounds for dimension 0 with size 5000",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m gt_primal_trainer \u001b[38;5;241m=\u001b[39m SamplingTrainer(gt_primal_model, optimizer\u001b[38;5;241m=\u001b[39mgt_primal_optimizer, loss_fn\u001b[38;5;241m=\u001b[39mreverse_loss(kl_divergence), metrics\u001b[38;5;241m=\u001b[39m[accuracy], data_sampler\u001b[38;5;241m=\u001b[39mgt_data_sampler)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing the ground truth mixture parameters on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mgt_primal_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/pythagoras-agi-new/src/engine/trainer.py:180\u001b[0m, in \u001b[0;36mSamplingTrainer.train\u001b[0;34m(self, num_samples, batch_size, average_window)\u001b[0m\n\u001b[1;32m    178\u001b[0m tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_sampler\u001b[38;5;241m.\u001b[39msample(this_batch_size)\n\u001b[1;32m    179\u001b[0m data_batch \u001b[38;5;241m=\u001b[39m LabeledDataBatch\u001b[38;5;241m.\u001b[39mread_data_batch(tensors, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 180\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m ra\u001b[38;5;241m.\u001b[39madd(result\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    183\u001b[0m update_norm_ra\u001b[38;5;241m.\u001b[39madd(result\u001b[38;5;241m.\u001b[39mscaled_step_norm\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[0;32m~/dev/pythagoras-agi-new/src/engine/trainer.py:87\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, data_batch)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m scaled_step_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     90\u001b[0m         [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TrainStepResult(\n\u001b[1;32m    102\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss, scaled_step_norm\u001b[38;5;241m=\u001b[39mscaled_step_norm, metrics\u001b[38;5;241m=\u001b[39mmetrics\n\u001b[1;32m    103\u001b[0m )\n",
            "File \u001b[0;32m~/dev/pythagoras-agi-new/src/models/mixture.py:221\u001b[0m, in \u001b[0;36mHomogenousMixtureModel.project_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproject_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixture_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_model\u001b[38;5;241m.\u001b[39mproject_parameters()\n",
            "File \u001b[0;32m~/dev/pythagoras-agi-new/src/models/mixture.py:178\u001b[0m, in \u001b[0;36mPrimalMixtureLayer.project_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproject_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixture_probs\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mproject_to_simplex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixture_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditional_probs\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m project_to_simplex(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditional_probs\u001b[38;5;241m.\u001b[39mdata, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    181\u001b[0m     )\n",
            "File \u001b[0;32m~/dev/pythagoras-agi-new/src/torch_utils.py:95\u001b[0m, in \u001b[0;36mproject_to_simplex\u001b[0;34m(x, dim)\u001b[0m\n\u001b[1;32m     93\u001b[0m t \u001b[38;5;241m=\u001b[39m (cumsum_sorted_x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m range_tensor\n\u001b[1;32m     94\u001b[0m k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(sorted_x \u001b[38;5;241m>\u001b[39m t, dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 95\u001b[0m tau \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mclamp(x \u001b[38;5;241m-\u001b[39m tau, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: index -1 is out of bounds for dimension 0 with size 5000"
          ]
        }
      ],
      "source": [
        "# Train the primal model to match the ground truth model\n",
        "gt_primal_optimizer = SGD(gt_primal_model.mixture_layer.parameters(), lr=1)\n",
        "gt_primal_trainer = SamplingTrainer(gt_primal_model, optimizer=gt_primal_optimizer, loss_fn=reverse_loss(kl_divergence), metrics=[accuracy_with_logits], data_sampler=gt_data_sampler)\n",
        "\n",
        "print(f\"Initializing the ground truth mixture parameters on {dataset_name}\")\n",
        "gt_primal_trainer.train(num_samples=1000000, batch_size=256, average_window=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "truncation_factor = 0.5\n",
        "weak_num_layers = 2\n",
        "weak_hidden_dim = 50\n",
        "wk_init_model = TruncatedMLP(input_dim=input_dim, hidden_dim=weak_hidden_dim, output_dim=len(train_dataset.classes), num_layers=weak_num_layers, truncation_factor=truncation_factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model device for training: cpu\n",
            "Using model device for training: cpu\n"
          ]
        }
      ],
      "source": [
        "wk_optimizer = Adam(wk_init_model.parameters(), lr=1e-3)\n",
        "wk_trainer = DatasetTrainer(wk_init_model, optimizer=wk_optimizer, loss_fn=cross_entropy_with_logits, dataset=train_dataset, metrics=[accuracy_with_logits])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing the weak model on mnist\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.45s/it, loss=0.279, grad_norm=1.54, acc=0.925]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Initializing the weak model on {dataset_name}\")\n",
        "wk_trainer.train(num_epochs=2, batch_size=64, average_window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Do the weak to strong transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup the ground truth model. We increase confidence via finetune_scale to lower ground-truth entropy\n",
        "with torch.no_grad():\n",
        "    gt_primal_model.mixture_layer.conditional_weights.data *= 1e3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(gt_primal_model.mixture_layer.mixture_weights.data, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "st_primal_model = HomogenousMixtureModel(\n",
        "    features_model=features_model,\n",
        "    output_dim=num_classes,\n",
        "    no_features_grad=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the weak model to use the trained weak representations\n",
        "wk_model = TruncatedMLP(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=weak_hidden_dim,\n",
        "    output_dim=num_classes,\n",
        "    num_layers=weak_num_layers,\n",
        "    representation_state_dict=wk_init_model.representation.state_dict(),\n",
        "    truncation_factor=wk_init_model.truncation_factor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the weak model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model device for training: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15625/15625 [00:40<00:00, 386.16it/s, loss=0.28] \n"
          ]
        }
      ],
      "source": [
        "# Train the weak model\n",
        "wk_train_num_samples = 1000000\n",
        "optimizer = AdamW(wk_model.finetune.parameters(), lr=1e-3)\n",
        "gt_to_wk_trainer = SamplingTrainer(model=wk_model, optimizer=optimizer, loss_fn=binary_cross_entropy, data_sampler=gt_data_sampler)\n",
        "gt_to_wk_trainer.train(num_samples=wk_train_num_samples, batch_size=64, average_window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the strong model using the weak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wk data sampler\n",
        "wk_data_sampler = SyntheticDataSampler(model=wk_model, input_dim=input_dim, output_dim=task_output_dim, var=var)\n",
        "wk_data_sampler.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model device for training: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 31250/31250 [01:29<00:00, 350.91it/s, loss=0.027] \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Train the strong model - this uses the proper loss function for our misfit inequality\n",
        "st_train_num_samples=2000000\n",
        "optimizer = AdamW(st_model.finetune.parameters(), lr=1e-3)\n",
        "st_to_wk_trainer = SamplingTrainer(model=st_model, optimizer=optimizer, loss_fn=reverse_loss(logistic_bregman_binary), data_sampler=wk_data_sampler, use_label_logits=True)\n",
        "st_to_wk_trainer.train(num_samples=st_train_num_samples, batch_size=64, average_window=10)\n",
        "\n",
        "# Train the strong model - this uses cross-entropy. It does not obey our misfit inequality, but still seems to work. Why?\n",
        "# st_train_num_samples=2000000\n",
        "# optimizer = AdamW(st_model.finetune.parameters(), lr=1e-3)\n",
        "# st_to_wk_trainer = SamplingTrainer(model=st_model, optimizer=optimizer, loss_fn=binary_cross_entropy, data_sampler=wk_data_sampler)\n",
        "# st_to_wk_trainer.train(num_samples=st_train_num_samples, batch_size=64, average_window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Estimate misfit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model device for synthetic data generation: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataBatch(x=tensor([[ 1.0738,  0.3794,  0.6472,  ...,  1.0327, -0.6142,  0.8089],\n",
              "        [-1.1780, -0.4867,  0.8217,  ..., -0.2647,  0.0819, -1.3179],\n",
              "        [-0.5802,  0.2627, -1.2674,  ...,  0.5013,  1.5323, -0.6210],\n",
              "        ...,\n",
              "        [ 0.2662, -1.5649,  1.1858,  ...,  0.3926,  0.5386, -0.6818],\n",
              "        [ 0.6524, -0.5471, -0.8080,  ..., -0.5471, -0.9330, -0.4172],\n",
              "        [-0.0048,  0.6548, -0.2040,  ..., -0.6937, -0.5767,  0.7211]]), y=tensor([[6.5902e-05, 3.0100e-01, 4.6389e-05, 9.9193e-05, 8.0595e-06, 1.7871e-04,\n",
              "         9.9318e-05, 1.5810e-04, 5.9491e-04, 6.9775e-01],\n",
              "        [6.6296e-04, 1.0963e-03, 1.9430e-04, 1.9886e-05, 2.6151e-04, 2.9914e-03,\n",
              "         1.5754e-06, 9.6555e-01, 1.6108e-04, 2.9058e-02],\n",
              "        [6.2568e-07, 5.7398e-01, 2.0274e-07, 6.0367e-07, 1.3372e-06, 5.4663e-06,\n",
              "         2.8296e-10, 8.1377e-08, 8.7887e-04, 4.2513e-01],\n",
              "        [1.2365e-03, 3.7693e-03, 6.1748e-01, 9.8452e-03, 4.8323e-02, 1.4312e-02,\n",
              "         3.1745e-04, 8.2711e-02, 1.0323e-01, 1.1878e-01],\n",
              "        [6.7261e-05, 2.7518e-06, 8.6282e-01, 6.7712e-04, 1.9164e-02, 4.2667e-03,\n",
              "         9.6712e-03, 4.5648e-02, 7.0884e-06, 5.7681e-02],\n",
              "        [5.5597e-04, 1.4833e-03, 1.7417e-01, 1.6352e-01, 1.0272e-03, 6.4141e-01,\n",
              "         2.7826e-05, 4.1876e-04, 8.8411e-04, 1.6501e-02],\n",
              "        [5.5986e-04, 9.5022e-02, 4.9635e-05, 3.9851e-05, 6.7518e-04, 4.5816e-05,\n",
              "         8.7794e-06, 4.1010e-04, 1.3373e-03, 9.0185e-01],\n",
              "        [3.1653e-04, 8.8775e-03, 2.7788e-03, 4.9172e-03, 8.3791e-05, 3.1832e-03,\n",
              "         1.8932e-01, 6.9724e-01, 1.5959e-03, 9.1688e-02],\n",
              "        [6.9118e-03, 2.0115e-02, 3.7281e-03, 1.0302e-03, 5.5360e-03, 2.4996e-02,\n",
              "         1.3466e-03, 6.6455e-01, 1.0937e-05, 2.7178e-01],\n",
              "        [1.0264e-03, 9.3655e-02, 5.9614e-03, 8.8954e-04, 1.5099e-02, 3.6806e-03,\n",
              "         1.0612e-05, 6.5480e-02, 1.6764e-05, 8.1418e-01]]), y_logits=tensor([[ -9.2133,  -0.7867,  -9.5645,  -8.8044, -11.3147,  -8.2157,  -8.8032,\n",
              "          -8.3383,  -7.0131,   0.0541],\n",
              "        [ -5.8457,  -5.3428,  -7.0731,  -9.3524,  -6.7760,  -4.3390, -11.8879,\n",
              "           1.4380,  -7.2605,  -2.0654],\n",
              "        [-12.4651,   1.2642, -13.5920, -12.5009, -11.7056, -10.2976, -20.1664,\n",
              "         -14.5048,  -5.2175,   0.9640],\n",
              "        [ -8.0729,  -6.9583,  -1.8595,  -5.9982,  -4.4072,  -5.6241,  -9.4326,\n",
              "          -3.8698,  -3.6482,  -3.5079],\n",
              "        [-10.8286, -14.0249,  -1.3692,  -8.5193,  -5.1763,  -6.6786,  -5.8602,\n",
              "          -4.3084, -13.0787,  -4.0745],\n",
              "        [ -7.8305,  -6.8492,  -2.0834,  -2.1465,  -7.2166,  -0.7797, -10.8252,\n",
              "          -8.1139,  -7.3666,  -4.4400],\n",
              "        [ -5.9932,  -0.8591,  -8.4163,  -8.6358,  -5.8060,  -8.4963, -10.1485,\n",
              "          -6.3045,  -5.1225,   1.3913],\n",
              "        [ -9.9894,  -6.6555,  -7.8170,  -7.2463, -11.3185,  -7.6812,  -3.5956,\n",
              "          -2.2919,  -8.3716,  -4.3206],\n",
              "        [ -6.7894,  -5.7212,  -7.4067,  -8.6928,  -7.0113,  -5.5039,  -8.4250,\n",
              "          -2.2235, -13.2382,  -3.1176],\n",
              "        [ -6.9582,  -2.4446,  -5.1989,  -7.1012,  -4.2695,  -5.6811, -11.5299,\n",
              "          -2.8024, -11.0727,  -0.2820]]))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gt_data_sampler = SyntheticDataSampler(model=gt_model, input_dim=input_dim, output_dim=task_output_dim, var=var)\n",
        "gt_data_sampler.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1000000/5093263872 [01:20<113:58:24, 12410.93it/s, gt_to_st_xe=1.68, gt_to_wk_xe=1.92, st_to_wk=0.235, wk_to_st=0.218, misfit_xe=-0.00133, gt_to_st=0.954, gt_to_wk=1.19, gt_ent=0.731, st_ent=1.99, wk_ent=1.81, misfit=-0.00133]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reached max samples 1000000.\n",
            "Errors: {'gt_to_st_xe': 0.0010214232606813312, 'gt_to_wk_xe': 0.0014456016942858696, 'st_to_wk': 0.00034318119287490845, 'wk_to_st': 0.0003097380104009062, 'misfit_xe': 0.000950468354858458, 'gt_to_st': 0.0008139380370266736, 'gt_to_wk': 0.0013162209652364254, 'gt_ent': 0.001011607819236815, 'st_ent': 0.00032130704494193196, 'wk_ent': 0.0005803710082545877, 'misfit': 0.000950468354858458}\n",
            "{'gt_ent__err': 0.001011607819236815,\n",
            " 'gt_ent__mean': 0.7306965589523315,\n",
            " 'gt_ent__std': 0.5058038830757141,\n",
            " 'gt_to_st__err': 0.0008139380370266736,\n",
            " 'gt_to_st__mean': 0.9535191059112549,\n",
            " 'gt_to_st__std': 0.40696901082992554,\n",
            " 'gt_to_st_xe__err': 0.0010214232606813312,\n",
            " 'gt_to_st_xe__mean': 1.6842186450958252,\n",
            " 'gt_to_st_xe__std': 0.5107116103172302,\n",
            " 'gt_to_wk__err': 0.0013162209652364254,\n",
            " 'gt_to_wk__mean': 1.1871635913848877,\n",
            " 'gt_to_wk__std': 0.6581104397773743,\n",
            " 'gt_to_wk_xe__err': 0.0014456016942858696,\n",
            " 'gt_to_wk_xe__mean': 1.9178581237792969,\n",
            " 'gt_to_wk_xe__std': 0.7228007912635803,\n",
            " 'misfit__err': 0.000950468354858458,\n",
            " 'misfit__mean': -0.0013318016426637769,\n",
            " 'misfit__std': 0.47523415088653564,\n",
            " 'misfit_xe__err': 0.000950468354858458,\n",
            " 'misfit_xe__mean': -0.00133180629927665,\n",
            " 'misfit_xe__std': 0.47523415088653564,\n",
            " 'st_ent__err': 0.00032130704494193196,\n",
            " 'st_ent__mean': 1.992904782295227,\n",
            " 'st_ent__std': 0.1606535166501999,\n",
            " 'st_to_wk__err': 0.00034318119287490845,\n",
            " 'st_to_wk__mean': 0.23497411608695984,\n",
            " 'st_to_wk__std': 0.17159058153629303,\n",
            " 'wk_ent__err': 0.0005803710082545877,\n",
            " 'wk_ent__mean': 1.8096582889556885,\n",
            " 'wk_ent__std': 0.29018548130989075,\n",
            " 'wk_to_st__err': 0.0003097380104009062,\n",
            " 'wk_to_st__mean': 0.21828371286392212,\n",
            " 'wk_to_st__std': 0.15486900508403778}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from src.measurements import EstimateKLMisfit\n",
        "\n",
        "misfit_estimator = EstimateKLMisfit(strong_model=st_model, weak_model=wk_model, data_sampler=gt_data_sampler)\n",
        "result = misfit_estimator.estimate(tolerance=1e-2, batch_size=64, max_samples=1000000)\n",
        "pprint(result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pagi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
