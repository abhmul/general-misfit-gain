{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from src.params import DatasetName, DEFAULT_DEVICE, CPU_GENERATOR\n",
    "from src.datasets import load_dataset, TRANSFORM_RGB, TRANSFORM_BASE, ModelEmbeddingsLabeledDataset\n",
    "from src.models import MLP, TruncatedMLP, PrimalThreshold, HomogenousMixtureModel, BagOfDecisionBoundaries, MeanThreshold\n",
    "from src.datasets import SyntheticNormalDataSampler, SyntheticDatasetDataSampler\n",
    "from src.engine import DatasetClassificationTrainer, SamplingClassificationTrainer, DatasetInference\n",
    "from src.metrics import Accuracy, KLDivergence, CrossEntropy\n",
    "from src.measurements import EstimatedLabeledModelLosses, EstimateWeakToStrong, LossSpec, StatSpec\n",
    "\n",
    "print(DEFAULT_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_name = DatasetName.CIFAR10\n",
    "\n",
    "transform = T.Compose([T.ToTensor(), T.Lambda(lambda x: torch.flatten(x)), T.Lambda(lambda x: (x - x.mean()) / x.std()),])\n",
    "\n",
    "num_classes = 10\n",
    "target_transform = lambda x: F.one_hot(torch.tensor(x), num_classes=10).to(torch.float32)\n",
    "\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\", transform=transform, target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 50000\n",
      "Number of classes in dataset: 10\n",
      "Sample dim: torch.Size([3072])\n",
      "Target shape: torch.Size([10])\n",
      "Sample Mean: -2.60770320892334e-08\n",
      "Sample Std: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get some stats on the mnist dataset \n",
    "print(f\"Number of samples: {len(train_dataset)}\")\n",
    "print(f\"Number of classes in dataset: {len(train_dataset.classes)}\")\n",
    "\n",
    "tmp_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "sample = next(iter(tmp_loader))\n",
    "\n",
    "data_sample_dim = sample[0].shape[1:]\n",
    "print(f\"Sample dim: {data_sample_dim}\")\n",
    "print(f\"Target shape: {sample[1].shape[1:]}\")\n",
    "print(f\"Sample Mean: {sample[0].mean()}\")\n",
    "print(f\"Sample Std: {sample[0].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_sample_dim[0]\n",
    "hidden_dim = 50\n",
    "target_num_layers = 2\n",
    "gt_init_model = MLP(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    num_layers=target_num_layers,\n",
    ")\n",
    "representation = gt_init_model.representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model device for training: cpu\n",
      "Using model device for training: cpu\n",
      "Initializing the strong representation on cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:22<00:00,  4.42s/it, loss=1.28, up_norm=0.0937, grad_norm=1.47, accuracy=0.536]\n"
     ]
    }
   ],
   "source": [
    "features_optimizer = Adam(gt_init_model.parameters(), lr=1e-3)\n",
    "loss_fn = CrossEntropy(output_logits=True, label_logits=False)\n",
    "accuracy_fn = Accuracy(output_logits=True, label_logits=False, hard=True)\n",
    "features_trainer = DatasetClassificationTrainer(\n",
    "    gt_init_model,\n",
    "    optimizer=features_optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=[accuracy_fn],\n",
    "    dataset=train_dataset,\n",
    ")\n",
    "\n",
    "print(f\"Initializing the strong representation on {dataset_name}\")\n",
    "features_trainer.train(num_epochs=5, batch_size=256, average_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb\n",
    "num_combinations = 3\n",
    "# Now train primal model\n",
    "# features_model = BagOfDecisionBoundaries(\n",
    "#     input_dim=hidden_dim, \n",
    "#     output_dim=comb(hidden_dim, num_combinations), \n",
    "#     num_states=2 ** num_combinations\n",
    "# ).prepend(representation, input_dim=input_dim, output_dim=hidden_dim)\n",
    "features_model = MeanThreshold(\n",
    "    input_dim=hidden_dim,\n",
    "    num_combinations=3\n",
    ").prepend(representation, input_dim=input_dim, output_dim=hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_model = PrimalThreshold(thresholds).prepend(gt_representation, input_dim=input_dim, output_dim=hidden_dim)\n",
    "primal_gt_model = HomogenousMixtureModel(\n",
    "    features_model=features_model,\n",
    "    output_dim=num_classes,\n",
    "    no_features_grad=True,\n",
    "    use_dual_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model device for training: cpu\n",
      "Using model device for training: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:38<00:00, 49.32s/it, loss=1.58, up_norm=1.72, grad_norm=0.0108, accuracy=0.464] \n"
     ]
    }
   ],
   "source": [
    "# Test training the primal model on the actual data\n",
    "primal_gt_optimizer = Adam(primal_gt_model.mixture_layer.parameters(), lr=1e-1)\n",
    "loss_fn = CrossEntropy(output_logits=False, label_logits=False)\n",
    "accuracy_fn = Accuracy(output_logits=False, label_logits=False)\n",
    "primal_trainer = DatasetClassificationTrainer(\n",
    "    primal_gt_model,\n",
    "    optimizer=primal_gt_optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=[accuracy_fn],\n",
    "    dataset=train_dataset,\n",
    ")\n",
    "\n",
    "primal_trainer.train(\n",
    "    num_epochs=2, batch_size=256, average_window=10, update_pbar_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the weak to strong transfer\n",
    "\n",
    "1. Train the weak model on the ground truth data\n",
    "2. Train a primal strong model on the weak model's predictions\n",
    "3. Also train a dual strong model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation_factor = 0.5\n",
    "weak_num_layers = 2\n",
    "weak_hidden_dim = 50\n",
    "wk_model = TruncatedMLP(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=weak_hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    num_layers=weak_num_layers,\n",
    "    truncation_factor=truncation_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model device for training: cpu\n",
      "Using model device for training: cpu\n",
      "Training the weak model on cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:20<00:00,  4.19s/it, loss=1.47, up_norm=0.0673, grad_norm=1.46, accuracy=0.475]\n"
     ]
    }
   ],
   "source": [
    "wk_optimizer = Adam(wk_model.parameters(), lr=1e-3)\n",
    "loss_fn = CrossEntropy(output_logits=True, label_logits=False)\n",
    "accuracy_fn = Accuracy(output_logits=True, label_logits=False, hard=True)\n",
    "wk_trainer = DatasetClassificationTrainer(\n",
    "    wk_model,\n",
    "    optimizer=wk_optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=[accuracy_fn],\n",
    "    dataset=train_dataset,\n",
    ")\n",
    "\n",
    "print(f\"Training the weak model on {dataset_name}\")\n",
    "wk_trainer.train(num_epochs=5, batch_size=256, average_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "primal_st_model = HomogenousMixtureModel(\n",
    "    features_model=features_model,\n",
    "    output_dim=num_classes,\n",
    "    no_features_grad=True,\n",
    "    use_dual_weights=True,\n",
    ")\n",
    "\n",
    "dual_st_model = MLP(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=num_classes,\n",
    "    num_layers=target_num_layers,\n",
    "    representation_state_dict=gt_init_model.representation.state_dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sanity-checks\n",
    "assert (\n",
    "    primal_gt_model.features_model.state_dict().keys()\n",
    "    == primal_st_model.features_model.state_dict().keys()\n",
    ")\n",
    "assert all(\n",
    "    [\n",
    "        torch.all(\n",
    "            primal_gt_model.features_model.state_dict()[k]\n",
    "            == primal_st_model.features_model.state_dict()[k]\n",
    "        )\n",
    "        for k in primal_st_model.features_model.state_dict().keys()\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert (\n",
    "    dual_st_model.state_dict().keys()\n",
    "    == gt_init_model.state_dict().keys()\n",
    ")\n",
    "assert all(\n",
    "    [\n",
    "        torch.all(\n",
    "            dual_st_model.representation.state_dict()[k]\n",
    "            == gt_init_model.representation.state_dict()[k]\n",
    "        )\n",
    "        for k in dual_st_model.representation.state_dict().keys()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model device for synthetic data generation: cpu\n"
     ]
    }
   ],
   "source": [
    "# Make a data sampler for the weak model\n",
    "wk_model_data_sampler = SyntheticDatasetDataSampler(model=wk_model, dataset=train_dataset, input_dim=input_dim, output_dim=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model device for training: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:22<00:00, 12.17it/s, loss=1.63, up_norm=0.00283, grad_norm=0.207, accuracy=0.645]\n"
     ]
    }
   ],
   "source": [
    "# Train the dual strong model on the weak model's outputs\n",
    "dual_st_optimizer = Adam(dual_st_model.finetune.parameters(), lr=1e-3)\n",
    "loss_fn = CrossEntropy(output_logits=True, label_logits=True)\n",
    "accuracy_fn = Accuracy(output_logits=True, label_logits=True, hard=True)\n",
    "dual_st_trainer = SamplingClassificationTrainer(\n",
    "    dual_st_model,\n",
    "    optimizer=dual_st_optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=[accuracy_fn],\n",
    "    data_sampler=wk_model_data_sampler,\n",
    ")\n",
    "\n",
    "batch_size = 2 ** 10\n",
    "dual_st_trainer.train(\n",
    "    num_samples=1000 * batch_size, batch_size=batch_size, average_window=10, update_pbar_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model device for training: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [02:20<00:00,  1.24it/s, loss=0.428, up_norm=1.6, grad_norm=0.00206, accuracy=0.538] \n"
     ]
    }
   ],
   "source": [
    "# Train the primal strong model on the weak model's outputs\n",
    "primal_st_optimizer = Adam(primal_st_model.mixture_layer.parameters(), lr=1e-1)\n",
    "loss_fn = KLDivergence(output_logits=False, label_logits=True)\n",
    "accuracy_fn = Accuracy(output_logits=False, label_logits=True, hard=True)\n",
    "primal_st_trainer = SamplingClassificationTrainer(\n",
    "    primal_st_model,\n",
    "    optimizer=primal_st_optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=[accuracy_fn],\n",
    "    data_sampler=wk_model_data_sampler,\n",
    ")\n",
    "\n",
    "batch_size = 2 ** 10\n",
    "primal_st_trainer.train(\n",
    "    num_samples=175 * batch_size, batch_size=batch_size, average_window=10, update_pbar_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "x, y, y_logits = wk_model_data_sampler.sample(2)\n",
    "print(torch.sum(primal_gt_model(x),dim=1))\n",
    "print(primal_gt_model.mixture_layer.num_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0006, 1.0007, 1.0006, 1.0009, 1.0008, 1.0006, 1.0009, 1.0008],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.softmax(\n",
    "    primal_gt_model.mixture_layer.conditional_weights, dim=0).view(\n",
    "    primal_gt_model.mixture_layer.output_dim, primal_gt_model.mixture_layer.input_dim, primal_gt_model.mixture_layer.num_states\n",
    ")\n",
    "print(a.sum(dim=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the misfit amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu for estimating losses between models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:46<00:00,  1.05it/s, dual_gt<-gt_cross_entropy=1.26, dual_st<-gt_cross_entropy=1.4, primal_gt<-gt_cross_entropy=1.58, primal_st<-gt_cross_entropy=1.63, wk<-gt_cross_entropy=1.41, dual_gt<-gt_accuracy=0.554, dual_st<-gt_accuracy=0.517, primal_gt<-gt_accuracy=0.472, primal_st<-gt_accuracy=0.436, wk<-gt_accuracy=0.501, primal_gt<-primal_gt_kl_divergence=9.27e-8, primal_gt<-primal_st_kl_divergence=0.0624, primal_gt<-wk_kl_divergence=0.496, primal_gt<-primal_gt_accuracy=1, primal_gt<-primal_st_accuracy=0.715, primal_gt<-wk_accuracy=0.524, dual_gt<-dual_gt_kl_divergence=2.98e-10, dual_gt<-dual_st_kl_divergence=0.144, dual_gt<-wk_kl_divergence=0.334, dual_gt<-dual_gt_accuracy=1, dual_gt<-dual_st_accuracy=0.75, dual_gt<-wk_accuracy=0.589, primal_st<-primal_st_kl_divergence=2.01e-7, primal_st<-wk_kl_divergence=0.43, primal_st<-primal_st_accuracy=1, primal_st<-wk_accuracy=0.54, dual_st<-dual_st_kl_divergence=-1.99e-10, dual_st<-wk_kl_divergence=0.23, dual_st<-dual_st_accuracy=1, dual_st<-wk_accuracy=0.643]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dual_gt<-dual_gt_accuracy__err': 0.0,\n",
      " 'dual_gt<-dual_gt_accuracy__mean': 1.0,\n",
      " 'dual_gt<-dual_gt_accuracy__std': 0.0,\n",
      " 'dual_gt<-dual_gt_kl_divergence__err': 2.1212414047511174e-09,\n",
      " 'dual_gt<-dual_gt_kl_divergence__mean': 2.983678304424586e-10,\n",
      " 'dual_gt<-dual_gt_kl_divergence__std': 2.3716199848422548e-07,\n",
      " 'dual_gt<-dual_st_accuracy__err': 0.003871781751513481,\n",
      " 'dual_gt<-dual_st_accuracy__mean': 0.7502401471138,\n",
      " 'dual_gt<-dual_st_accuracy__std': 0.4328783452510834,\n",
      " 'dual_gt<-dual_st_kl_divergence__err': 0.0009704749099910259,\n",
      " 'dual_gt<-dual_st_kl_divergence__mean': 0.1443948596715927,\n",
      " 'dual_gt<-dual_st_kl_divergence__std': 0.10850239545106888,\n",
      " 'dual_gt<-gt_accuracy__err': 0.004445788450539112,\n",
      " 'dual_gt<-gt_accuracy__mean': 0.5542400479316711,\n",
      " 'dual_gt<-gt_accuracy__std': 0.4970542788505554,\n",
      " 'dual_gt<-gt_cross_entropy__err': 0.009310591965913773,\n",
      " 'dual_gt<-gt_cross_entropy__mean': 1.2554808855056763,\n",
      " 'dual_gt<-gt_cross_entropy__std': 1.0409557819366455,\n",
      " 'dual_gt<-wk_accuracy__err': 0.004400276113301516,\n",
      " 'dual_gt<-wk_accuracy__mean': 0.5892999768257141,\n",
      " 'dual_gt<-wk_accuracy__std': 0.4919658303260803,\n",
      " 'dual_gt<-wk_kl_divergence__err': 0.0029221782460808754,\n",
      " 'dual_gt<-wk_kl_divergence__mean': 0.33364975452423096,\n",
      " 'dual_gt<-wk_kl_divergence__std': 0.32670944929122925,\n",
      " 'dual_st<-dual_st_accuracy__err': 0.0,\n",
      " 'dual_st<-dual_st_accuracy__mean': 1.0,\n",
      " 'dual_st<-dual_st_accuracy__std': 0.0,\n",
      " 'dual_st<-dual_st_kl_divergence__err': 2.5065554076775243e-09,\n",
      " 'dual_st<-dual_st_kl_divergence__mean': -1.9893067748633086e-10,\n",
      " 'dual_st<-dual_st_kl_divergence__std': 2.802414087454963e-07,\n",
      " 'dual_st<-gt_accuracy__err': 0.004469576757401228,\n",
      " 'dual_st<-gt_accuracy__mean': 0.5170601010322571,\n",
      " 'dual_st<-gt_accuracy__std': 0.49971386790275574,\n",
      " 'dual_st<-gt_cross_entropy__err': 0.007481304928660393,\n",
      " 'dual_st<-gt_cross_entropy__mean': 1.4015921354293823,\n",
      " 'dual_st<-gt_cross_entropy__std': 0.8364353179931641,\n",
      " 'dual_st<-wk_accuracy__err': 0.004284253343939781,\n",
      " 'dual_st<-wk_accuracy__mean': 0.6434199213981628,\n",
      " 'dual_st<-wk_accuracy__std': 0.47899407148361206,\n",
      " 'dual_st<-wk_kl_divergence__err': 0.001912274514324963,\n",
      " 'dual_st<-wk_kl_divergence__mean': 0.230160653591156,\n",
      " 'dual_st<-wk_kl_divergence__std': 0.21379879117012024,\n",
      " 'primal_gt<-gt_accuracy__err': 0.004464981146156788,\n",
      " 'primal_gt<-gt_accuracy__mean': 0.4716399908065796,\n",
      " 'primal_gt<-gt_accuracy__std': 0.4992000460624695,\n",
      " 'primal_gt<-gt_cross_entropy__err': 0.006066704168915749,\n",
      " 'primal_gt<-gt_cross_entropy__mean': 1.5772366523742676,\n",
      " 'primal_gt<-gt_cross_entropy__std': 0.6782781481742859,\n",
      " 'primal_gt<-primal_gt_accuracy__err': 0.0,\n",
      " 'primal_gt<-primal_gt_accuracy__mean': 1.0,\n",
      " 'primal_gt<-primal_gt_accuracy__std': 0.0,\n",
      " 'primal_gt<-primal_gt_kl_divergence__err': 1.7012754538470176e-09,\n",
      " 'primal_gt<-primal_gt_kl_divergence__mean': 9.274484114030201e-08,\n",
      " 'primal_gt<-primal_gt_kl_divergence__std': 1.9020838237793214e-07,\n",
      " 'primal_gt<-primal_st_accuracy__err': 0.004037187900394201,\n",
      " 'primal_gt<-primal_st_accuracy__mean': 0.7150999903678894,\n",
      " 'primal_gt<-primal_st_accuracy__std': 0.45137134194374084,\n",
      " 'primal_gt<-primal_st_kl_divergence__err': 0.00033438680111430585,\n",
      " 'primal_gt<-primal_st_kl_divergence__mean': 0.062419913709163666,\n",
      " 'primal_gt<-primal_st_kl_divergence__std': 0.03738557919859886,\n",
      " 'primal_gt<-wk_accuracy__err': 0.0044669825583696365,\n",
      " 'primal_gt<-wk_accuracy__mean': 0.5240998864173889,\n",
      " 'primal_gt<-wk_accuracy__std': 0.4994238317012787,\n",
      " 'primal_gt<-wk_kl_divergence__err': 0.0033669685944914818,\n",
      " 'primal_gt<-wk_kl_divergence__mean': 0.49611330032348633,\n",
      " 'primal_gt<-wk_kl_divergence__std': 0.37643852829933167,\n",
      " 'primal_st<-gt_accuracy__err': 0.004435739014297724,\n",
      " 'primal_st<-gt_accuracy__mean': 0.43629997968673706,\n",
      " 'primal_st<-gt_accuracy__std': 0.49593067169189453,\n",
      " 'primal_st<-gt_cross_entropy__err': 0.006399872247129679,\n",
      " 'primal_st<-gt_cross_entropy__mean': 1.6289705038070679,\n",
      " 'primal_st<-gt_cross_entropy__std': 0.7155274748802185,\n",
      " 'primal_st<-primal_st_accuracy__err': 0.0,\n",
      " 'primal_st<-primal_st_accuracy__mean': 1.0,\n",
      " 'primal_st<-primal_st_accuracy__std': 0.0,\n",
      " 'primal_st<-primal_st_kl_divergence__err': 1.6629400079182233e-09,\n",
      " 'primal_st<-primal_st_kl_divergence__mean': 2.0060416261458158e-07,\n",
      " 'primal_st<-primal_st_kl_divergence__std': 1.8592234596326307e-07,\n",
      " 'primal_st<-wk_accuracy__err': 0.004457890056073666,\n",
      " 'primal_st<-wk_accuracy__mean': 0.5399399995803833,\n",
      " 'primal_st<-wk_accuracy__std': 0.498407244682312,\n",
      " 'primal_st<-wk_kl_divergence__err': 0.0031199941877275705,\n",
      " 'primal_st<-wk_kl_divergence__mean': 0.42955130338668823,\n",
      " 'primal_st<-wk_kl_divergence__std': 0.34882596135139465,\n",
      " 'wk<-gt_accuracy__err': 0.004472177941352129,\n",
      " 'wk<-gt_accuracy__mean': 0.500540018081665,\n",
      " 'wk<-gt_accuracy__std': 0.5000047087669373,\n",
      " 'wk<-gt_cross_entropy__err': 0.009315193630754948,\n",
      " 'wk<-gt_cross_entropy__mean': 1.414469838142395,\n",
      " 'wk<-gt_cross_entropy__std': 1.0414702892303467}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "strong_models_dict = {\n",
    "    \"dual_gt\": gt_init_model,\n",
    "    \"dual_st\": dual_st_model,\n",
    "    \"primal_gt\": primal_gt_model,\n",
    "    \"primal_st\": primal_st_model,\n",
    "}\n",
    "weak_models_dict = {\n",
    "    \"wk\": wk_model,\n",
    "}\n",
    "models_dict = {**strong_models_dict, **weak_models_dict}\n",
    "\n",
    "losses_list = [\n",
    "    # WRT the ground truth labels\n",
    "    LossSpec(\n",
    "        name1=\"dual_gt\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=CrossEntropy(output_logits=True, label_logits=False),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_st\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=CrossEntropy(output_logits=True, label_logits=False),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_gt\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=CrossEntropy(output_logits=False, label_logits=False),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_st\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=CrossEntropy(output_logits=False, label_logits=False),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"wk\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=CrossEntropy(output_logits=True, label_logits=False),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_gt\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=Accuracy(output_logits=True, label_logits=False, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_st\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=Accuracy(output_logits=True, label_logits=False, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_gt\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=Accuracy(output_logits=False, label_logits=False, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_st\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=Accuracy(output_logits=False, label_logits=False, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"wk\",\n",
    "        name2=EstimateWeakToStrong.GT,\n",
    "        loss_fn=Accuracy(output_logits=True, label_logits=False, hard=True),\n",
    "    ),\n",
    "    # WRT the primal_gt model\n",
    "    LossSpec( # A sanity check -- should be 0\n",
    "        name1=\"primal_gt\",\n",
    "        name2=\"primal_gt\",\n",
    "        loss_fn=KLDivergence(output_logits=False, label_logits=False),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_gt\",\n",
    "        name2=\"primal_st\",\n",
    "        loss_fn=KLDivergence(output_logits=False, label_logits=False),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_gt\",\n",
    "        name2=\"wk\",\n",
    "        loss_fn=KLDivergence(output_logits=False, label_logits=True),\n",
    "    ),\n",
    "    LossSpec( # A sanity check -- should be 1\n",
    "        name1=\"primal_gt\",\n",
    "        name2=\"primal_gt\",\n",
    "        loss_fn=Accuracy(output_logits=False, label_logits=False, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_gt\",\n",
    "        name2=\"primal_st\",\n",
    "        loss_fn=Accuracy(output_logits=False, label_logits=False, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_gt\",\n",
    "        name2=\"wk\",\n",
    "        loss_fn=Accuracy(output_logits=False, label_logits=True, hard=True),\n",
    "    ),\n",
    "    # WRT the dual_gt model\n",
    "    LossSpec( # A sanity check -- should be 0\n",
    "        name1=\"dual_gt\",\n",
    "        name2=\"dual_gt\",\n",
    "        loss_fn=KLDivergence(output_logits=True, label_logits=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_gt\",\n",
    "        name2=\"dual_st\",\n",
    "        loss_fn=KLDivergence(output_logits=True, label_logits=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_gt\",\n",
    "        name2=\"wk\",\n",
    "        loss_fn=KLDivergence(output_logits=True, label_logits=True),\n",
    "    ),\n",
    "    LossSpec( # A sanity check -- should be 1\n",
    "        name1=\"dual_gt\",\n",
    "        name2=\"dual_gt\",\n",
    "        loss_fn=Accuracy(output_logits=True, label_logits=True, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_gt\",\n",
    "        name2=\"dual_st\",\n",
    "        loss_fn=Accuracy(output_logits=True, label_logits=True, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_gt\",\n",
    "        name2=\"wk\",\n",
    "        loss_fn=Accuracy(output_logits=True, label_logits=True, hard=True),\n",
    "    ),\n",
    "    # WRT the primal_st model\n",
    "    LossSpec( # A sanity check -- should be 0\n",
    "        name1=\"primal_st\",\n",
    "        name2=\"primal_st\",\n",
    "        loss_fn=KLDivergence(output_logits=False, label_logits=False),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_st\",\n",
    "        name2=\"wk\",\n",
    "        loss_fn=KLDivergence(output_logits=False, label_logits=True),\n",
    "    ),\n",
    "    LossSpec( # A sanity check -- should be 1\n",
    "        name1=\"primal_st\",\n",
    "        name2=\"primal_st\",\n",
    "        loss_fn=Accuracy(output_logits=False, label_logits=False, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"primal_st\",\n",
    "        name2=\"wk\",\n",
    "        loss_fn=Accuracy(output_logits=False, label_logits=True, hard=True),\n",
    "    ),\n",
    "    # WRT the dual_st model\n",
    "    LossSpec( # A sanity check -- should be 0\n",
    "        name1=\"dual_st\",\n",
    "        name2=\"dual_st\",\n",
    "        loss_fn=KLDivergence(output_logits=True, label_logits=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_st\",\n",
    "        name2=\"wk\",\n",
    "        loss_fn=KLDivergence(output_logits=True, label_logits=True),\n",
    "    ),\n",
    "    LossSpec( # A sanity check -- should be 1\n",
    "        name1=\"dual_st\",\n",
    "        name2=\"dual_st\",\n",
    "        loss_fn=Accuracy(output_logits=True, label_logits=True, hard=True),\n",
    "    ),\n",
    "    LossSpec(\n",
    "        name1=\"dual_st\",\n",
    "        name2=\"wk\",\n",
    "        loss_fn=Accuracy(output_logits=True, label_logits=True, hard=True),\n",
    "    ),    \n",
    "]\n",
    "\n",
    "estimator = EstimatedLabeledModelLosses(\n",
    "    dataset=train_dataset,\n",
    "    models=models_dict,\n",
    "    losses=losses_list,\n",
    ")\n",
    "\n",
    "results = estimator.estimate(batch_size=2 ** 10)\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'primal_gt<-primal_st_kl_divergence__mean': 0.10186149179935455,\n",
    "'primal_gt<-wk_kl_divergence__mean': 0.41966670751571655,\n",
    " 'primal_st<-wk_kl_divergence__mean': 0.3058554232120514,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11381128430366516"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.41966670751571655 - 0.3058554232120514"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pagi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
